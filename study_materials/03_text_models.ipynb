{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP III - Transformer Based Text Models\n",
    "\n",
    "## Hugging Face\n",
    "\n",
    "Hugging Face is a platform for AI, two main parts: Hugging Face Transformers and Hugging Face Hub.\n",
    "- **Transformers**: An open-source Python library for Natural Language Processing (NLP) that provides pre-trained models and tools for building and training custom models.\n",
    "- **Hub**: A platform for sharing and discovering machine learning models, datasets, and other resources. \"Like Github for ML.\"\n",
    "\n",
    "[More details on Hugging Face for beginners](https://huggingface.co/blog/noob_intro_transformers)\n",
    "\n",
    "There are a wide range of pre-trained models available on Hugging Face, we can easily load them, use them and fine-tune them with the help of the Transformers library.\n",
    "\n",
    "[Hugging Face Text Models](https://huggingface.co/docs/transformers/en/model_doc/bert)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder vs Decoder\n",
    "\n",
    "<img src=\"../img/03_text_models/encoder_decoder.png\" width=\"600\">\n",
    "\n",
    "[Source](https://towardsdatascience.com/a-complete-guide-to-bert-with-code-9f87602e4a11/)\n",
    "\n",
    "Key differences in the architecture of encoder and decoder:\n",
    "* Encoder:\n",
    "  * Bi-directional self attention - attends to all tokens in the input sequence (no masking)\n",
    "* Decoder:\n",
    "  * Unidirectional self attention - attends to all tokens in the input sequence, but only to the left (causal masking)\n",
    "  * Cross attention - attends to the encoder output \n",
    "    * not used in decoder-only models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder-only Models\n",
    "\n",
    "* BERT (Bidirectional Encoder Representations from Transformers)\n",
    "* RoBERTa (Robustly Optimized BERT Pretraining Approach)\n",
    "* DistilBERT\n",
    "* ALBERT (A Lite BERT)\n",
    "* DPR (Dense Passage Retrieval)\n",
    "* \n",
    "\n",
    "### BERT Architecture\n",
    "\n",
    "\n",
    "https://huggingface.co/learn/llm-course/chapter7/7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/a-complete-guide-to-bert-with-code-9f87602e4a11/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Large Language Models\n",
    "\n",
    "A Language Model is a statistical or machine learning model that predicts the likelihood of a sequence of words or generates text based on learned patterns in language. The input to a language model is typically a sequence of words or tokens, and the output is either the probability distribution over the next word or the generated next word or token.\n",
    "\n",
    "A Large Language Model is a Language Model with an extremely high parameter number."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
