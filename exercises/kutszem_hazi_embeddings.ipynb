{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea30224e",
   "metadata": {},
   "source": [
    "# Embeddings HW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962266e3",
   "metadata": {},
   "source": [
    "## Exercise 1: Word analogies\n",
    "\n",
    "An analogy explains one thing in terms of another to highlight the ways in which they are alike. For example, *paris* is similar to *france* in the same way that *rome* is to *italy*. Word2Vec vectors sometimes shows the ability of solving analogy problem of the form **a is to b as a* is to what?**.\n",
    "\n",
    "In the cell below, we show you how to use word vectors to find x. The `most_similar` function finds words that are most similar to the words in the `positive` list and most dissimilar from the words in the `negative` list. The answer to the analogy will be the word ranked most similar (largest numerical value). In the case below, the top one word *italy* is the answer, so this analogy is solved successfully.\n",
    "\n",
    "**Your task** is to look for one analogy that can be solved successfully and one analogy that could not be solved using this pre-trained Word2Vec model. You can check out [this paper](https://www.semanticscholar.org/paper/Efficient-Estimation-of-Word-Representations-in-Mikolov-Chen/330da625c15427c6e42ccfa3b747fb29e5835bf0) for inspirations.\n",
    "\n",
    "Please only submit a nice and easy to read notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1bd706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecf160d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b348ba68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fprint_pairs(pairs: list):\n",
    "    \"\"\"Formatted print function on list of pairs\"\"\"\n",
    "    \n",
    "    print(\"-\"*38)\n",
    "    print(\"{: ^4} {: ^19} {: ^13}\".format(\"Rank\", \"Word\", \"Similarity\"))\n",
    "    print(\"-\"*4,\"-\"*19,\"-\"*13)\n",
    "\n",
    "    i = 1\n",
    "    for k,v in pairs:\n",
    "        print(\"{: <4} {: <19} {:>13.2f}\".format(i,k,v))\n",
    "        i += 1\n",
    "    \n",
    "    print(\"-\"*38)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b697bd12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 66.94558310508728 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Load 3 million Word2Vec Vectors, pre-trained on Google news, each with the dimension of 300\n",
    "# This model may take a few minutes to load for the first time.\n",
    "\n",
    "start_time = time.time()\n",
    "w2v_google = api.load(\"word2vec-google-news-300\")\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "08dbcd26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded vocab size: 3000000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loaded vocab size: {len(w2v_google.index_to_key)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd101012",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_google[\"cat\"].shape  # Embedding vector length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df3e261d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "Rank        Word          Similarity  \n",
      "---- ------------------- -------------\n",
      "1    italy                        0.52\n",
      "2    european                     0.51\n",
      "3    italian                      0.51\n",
      "4    epl                          0.49\n",
      "5    spain                        0.49\n",
      "6    england                      0.49\n",
      "7    italians                     0.48\n",
      "8    kosovo                       0.48\n",
      "9    lampard                      0.48\n",
      "10   malta                        0.48\n",
      "--------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run this cell to answer the analogy -- paris : france :: rome : x\n",
    "similarity_paris = w2v_google.most_similar(positive=['rome', 'france'], negative=['paris'])\n",
    "\n",
    "fprint_pairs(similarity_paris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1da5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bdd7d7",
   "metadata": {},
   "source": [
    "## Exercise 2: Classification (OPTIONAL)\n",
    "\n",
    "Do the data processing part of a classification with a simple neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38db9784",
   "metadata": {},
   "source": [
    "### Download data\n",
    "\n",
    "[Data description](https://www.kaggle.com/datasets/parulpandey/emotion-dataset)\n",
    "\n",
    "You can download the data from the web by using the link, or from code following the instructions above:\n",
    "\n",
    "1. Go to [Kaggle](https://www.kaggle.com) and register\n",
    "2. In your profile settings scroll down to API\n",
    "3. Generate key with: `Create New Token`. This downloads a file named `kaggle.json`\n",
    "4. Place it to the [appropriate location](https://github.com/Kaggle/kaggle-api/blob/main/docs/README.md#api-credentials)\n",
    "5. Now, you can download all the necessary files from code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d41901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install kaggle, torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c922f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "3c342422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/barbara/.config/kaggle/kaggle.json'\n",
      "Dataset URL: https://www.kaggle.com/datasets/parulpandey/emotion-dataset\n"
     ]
    }
   ],
   "source": [
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "dataset_slug = 'parulpandey/emotion-dataset'  # Dataset\n",
    "download_path = os.path.join(\"..\", \"data\", \"emotion_dataset\")  # Destination folder\n",
    "\n",
    "api.dataset_download_files(dataset_slug, path=download_path, unzip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5719a957",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['training.csv', 'test.csv', 'validation.csv']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(download_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9521202e",
   "metadata": {},
   "source": [
    "### Process data\n",
    "\n",
    "Make train and test datasets. Note that this dataset has 5 different labels: sadness (0), joy (1), love (2), anger (3), fear (4). Keep only rows with label 0 or 1 filter everything else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd84858d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10028, 10028, 1276, 1276)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Add your code here\n",
    "\n",
    "len(X_train), len(y_train), len(X_test), len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1ce045",
   "metadata": {},
   "source": [
    "### Convert the text to vector\n",
    "\n",
    "Do some kind of preprocessing and average the word vectors use your favorite library e.g. spacy, gensim. It can take a few seconds, save the final vectors for later so you don't have to wait all the time.\n",
    "\n",
    "FOr the example outputs, I used the gensim library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e292860d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2vec(text: str) -> list:\n",
    "    # TODO: Add your code here\n",
    "    pass\n",
    "\n",
    "    return list(sentence_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "2c3612cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor([text2vec(text_input) for text_input in X_train])\n",
    "y_train = torch.tensor(y_train)\n",
    "\n",
    "X_test = torch.tensor([text2vec(text_input) for text_input in X_test])\n",
    "y_test = torch.tensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "96283fc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10028, 300]),\n",
       " torch.Size([10028]),\n",
       " torch.Size([1276, 300]),\n",
       " torch.Size([1276]))"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.size(), y_train.size(), X_test.size(), y_test.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba62af99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "torch.save({\"X_train\": X_train, \"y_train\": y_train}, 'train_data.pth')\n",
    "torch.save({\"X_test\": X_test, \"y_train\": y_train}, 'test_data.pth')\n",
    "\n",
    "# Load\n",
    "# train_data = torch.load('train_data.pth')\n",
    "# X_train = train_data[\"X_train\"]\n",
    "# y_train = train_data[\"y_train\"]\n",
    "\n",
    "\n",
    "# test_data = torch.load('test_data.pth')\n",
    "# X_test = test_data[\"X_test\"]\n",
    "# y_train = test_data[\"y_train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569e1930",
   "metadata": {},
   "source": [
    "### Data loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "7dcb74df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f67055f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(dataset=TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=TensorDataset(X_test, y_test), batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d05d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1:\n",
      "X_batch shape: torch.Size([32, 300])\n",
      "y_batch shape: torch.Size([32])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the DataLoader\n",
    "\n",
    "for i, (X_batch, y_batch) in enumerate(train_loader):\n",
    "    print(f\"Batch {i+1}:\")\n",
    "    print(\"X_batch shape:\", X_batch.shape)\n",
    "    print(\"y_batch shape:\", y_batch.shape)\n",
    "    print()\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4859e8e7",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "00e44a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionClassifier(torch.nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(EmotionClassifier, self).__init__()\n",
    "\n",
    "        self.fc = torch.nn.Linear(input_dim, 1024)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.fc2 = torch.nn.Linear(1024, 512)\n",
    "        self.relu2 = torch.nn.ReLU()\n",
    "        self.fc3 = torch.nn.Linear(512, 256)\n",
    "        self.relu3 = torch.nn.ReLU()\n",
    "        self.fc4 = torch.nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.fc4(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a2d5e8",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "2ac8ff32",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EmotionClassifier(input_dim=300, num_classes=2)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1729af0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.3506, Accuracy: 0.8427\n",
      "Epoch [2/5], Loss: 0.2694, Accuracy: 0.8857\n",
      "Epoch [3/5], Loss: 0.2396, Accuracy: 0.8985\n",
      "Epoch [4/5], Loss: 0.2112, Accuracy: 0.9102\n",
      "Epoch [5/5], Loss: 0.1767, Accuracy: 0.9225\n"
     ]
    }
   ],
   "source": [
    "def train(model, train_loader, criterion, optimizer, device, num_epochs=5):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        total_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            predicted_labels = torch.argmax(outputs.data, dim=1)\n",
    "\n",
    "            total += y_batch.size(0)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            correct += (predicted_labels == y_batch).sum().item()\n",
    "        \n",
    "        print(f\"Epoch [{epoch}/{num_epochs}], Loss: {total_loss/len(train_loader):.4f}, Accuracy: {correct/total:.4f}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "model = train(model, train_loader, criterion, optimizer, device, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3c9ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.2455, Test Accuracy: 0.9013\n"
     ]
    }
   ],
   "source": [
    "def eval(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "\n",
    "            predicted_labels = torch.argmax(outputs.data, dim=1)\n",
    "\n",
    "            total += y_batch.size(0)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            correct += (predicted_labels == y_batch).sum().item()\n",
    "\n",
    "    return total_loss / len(test_loader), correct / total\n",
    "\n",
    "test_loss, test_accuracy = eval(model, test_loader, criterion, device)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "fe3fa9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_example(model, sentence, device):\n",
    "    sentence_vector = torch.tensor(text2vec(sentence)).unsqueeze(0).to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(sentence_vector)\n",
    "        predicted_label = torch.argmax(output.data, dim=1)\n",
    "        confidence = torch.nn.functional.softmax(output, dim=1)[0][predicted_label]\n",
    "        \n",
    "        print(f\"Input sentence: {sentence}\")\n",
    "        print(f\"Predicted label: {predicted_label.item()}\")\n",
    "        print(f\"Confidence: {confidence.item():.3f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "c6d24912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence: i am quite happy to be here today with you all\n",
      "Predicted label: 1\n",
      "Confidence: 1.000\n",
      "\n",
      "Input sentence: i am quite happy to be here today with you all, what a shame that it's raining\n",
      "Predicted label: 0\n",
      "Confidence: 0.559\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence_0 = \"i am quite happy to be here today with you all\"\n",
    "sentence_1 = \"i am quite happy to be here today with you all, what a shame that it's raining\"\n",
    "\n",
    "eval_example(model, sentence_0, device)\n",
    "eval_example(model, sentence_1, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
