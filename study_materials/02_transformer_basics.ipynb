{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP II. - Transformer Basics\n",
    "## Sequence to Sequence\n",
    "\n",
    "Sometimes we want to process sequential data: the elements of the sequence has temporal information, that is the order of the input matters.\n",
    "\n",
    "<img src=\"../img/02_transformer_basics/seq2seq.png\" width=\"800\">\n",
    "\n",
    "[source](https://karpathy.github.io/2015/05/21/rnn-effectiveness/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer\n",
    "\n",
    "The Transformer is a Deeplearning model architecture from 2017, originally designed for language translation. It consists of two parts: \n",
    "* The Encoder\n",
    "  * inputs the tokenized source language text and \n",
    "  * produces a vector representation of the whole input text (it can be considered as a text embedding, notice that so far we only considered the embedding of words separately, not whole texts).\n",
    "* The Decoder \n",
    "  * inputs the embedding of the input text and the tokenized first few words of the translated sentence and \n",
    "  * outputs a probability distribution over the target language vocabulary to predict the next word (token) of the translated sentence.\n",
    "    * It means that it is an auto-regressive model, that is to generate the whole sentence we have to feed the previous output and use it to generate the next word. The generation stops when we reach a maximum length or the generated token is the \\<EOS\\> flag (End Of Sentence). To start the generation we first feed the \\<SOS\\> (Start Of Sentence) special token, so we don't have to guess the first word of the sentence. In the tokenized training data we also add a \\<SOS\\> and an \\<EOS\\> token to the start and the end of each sentence.\n",
    "\n",
    "<img src=\"../img/02_transformer_basics/transformer_architecture.png\" width=\"400\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings\n",
    "\n",
    "The first component of the model is the embedding layer to encode the tokens. During the training it will learn it's own embeddings for each token, we don't use pre-trained embeddings (that's why the input is simply the embedding tokens)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projection and Softmax\n",
    "\n",
    "We use a linear layer at the end to transform the output of the previous hidden layer (we call this the dimension of the transformer model) to the dimension of the vocabulary, we call these the `logits`. Then we use the Softmax function to convert the logits to a probability distribution.\n",
    "\n",
    "<img src=\"../img/02_transformer_basics/softmax.png\" height=\"300\">\n",
    "<img src=\"../img/02_transformer_basics/exp.png\" height=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def softmax(logits: torch.Tensor) -> torch.Tensor:\n",
    "    norm = (torch.exp(logits)).sum(dim=1)\n",
    "    norm = norm.unsqueeze(dim=1)\n",
    "\n",
    "    probabilities = torch.exp(logits) / norm\n",
    "\n",
    "    return torch.round(probabilities, decimals=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0200, 0.9000, 0.0500, 0.0100, 0.0200]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = torch.tensor([[1.3, 5.1, 2.2, 0.7, 1.1]])\n",
    "softmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled Dot-Product Attention\n",
    "\n",
    "$\\text{Attention}(Q, K, V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V$\n",
    "\n",
    "If $Q$ and $K$ are vectors, then $QK^T$ is basically the dot-product of the two vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 55.46487808227539 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Load 3 million Word2Vec Vectors, pre-trained on Google news, each with the dimension of 300\n",
    "# This model may take a few minutes to load.\n",
    "\n",
    "import gensim.downloader as api\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "w2v_google = api.load(\"word2vec-google-news-300\")\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4917/3878142539.py:8: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  QUERIES = torch.tensor([w2v_google[word] for word in query_words])  # (3, 300)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 300])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "query_words = [\"seal\", \"camel\", \"bear\"]\n",
    "key_words = [\"ice\", \"hot\", \"cold\"]\n",
    "VALUES = torch.tensor([10, 10, 5], dtype=torch.float32)\n",
    "\n",
    "\n",
    "QUERIES = torch.tensor([w2v_google[word] for word in query_words])  # (3, 300)\n",
    "KEYS = torch.tensor([w2v_google[word] for word in key_words])    # (3, 300)\n",
    "\n",
    "QUERIES.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|       | k1 | k2 | k3 |\n",
    "| ----- | -- | -- | -- |\n",
    "|**q1** |1.58|0.89|0.79|\n",
    "|**q2** |0.49|0.89|0.80|\n",
    "|**q3** |1.20|0.52|1.11|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.5779, 0.8922, 0.7857],\n",
       "        [0.4927, 0.8886, 0.8000],\n",
       "        [1.1997, 0.5245, 1.1086]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# similarities = QUERIES @ KEYS.T\n",
    "\n",
    "similarities = torch.zeros(size=(3, 3))\n",
    "\n",
    "for i, query in enumerate(QUERIES):\n",
    "    for j, key in enumerate(KEYS):\n",
    "        prod = query*key\n",
    "        dot_prod = prod.sum()\n",
    "\n",
    "        similarities[i,j] = dot_prod\n",
    "\n",
    "similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4300, 0.2078, 0.3664])\n",
      "tensor([0.0248, 0.0120, 0.0212])\n"
     ]
    }
   ],
   "source": [
    "print(similarities.std(dim=1))\n",
    "\n",
    "scaled_similarities = similarities / (300**0.5)\n",
    "print(scaled_similarities.std(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3400, 0.3300, 0.3300],\n",
       "        [0.3300, 0.3400, 0.3300],\n",
       "        [0.3400, 0.3300, 0.3400]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = softmax(scaled_similarities)  # TODO: Is dimension good?\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8.3500, 8.3500, 8.4000])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output = weights @ VALUES\n",
    "\n",
    "output = torch.zeros(size=(3,))\n",
    "\n",
    "for i, weight in enumerate(weights):\n",
    "    weighted_sum = (weight*VALUES).sum()\n",
    "    output[i] = weighted_sum\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Attention\n",
    "\n",
    "The Key, Query and Value vectors are coming from the same embeddings and we transform them with different Linear Layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 300])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = [\"the\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"lazy\", \"dog\"]\n",
    "embeddings = torch.tensor([w2v_google[word] for word in words])\n",
    "\n",
    "embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = torch.nn.Linear(300, 300)\n",
    "Q = torch.nn.Linear(300, 300)\n",
    "V = torch.nn.Linear(300, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 300])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def SelfAttention(K, Q, V, embeddings):\n",
    "    key = K(embeddings)\n",
    "    query = Q(embeddings)\n",
    "    value = V(embeddings)\n",
    "\n",
    "    batch_size, d_k = key.size()\n",
    "\n",
    "    scaled_similarities = (query @ key.T) / (d_k**0.5)\n",
    "    weights = softmax(scaled_similarities)\n",
    "\n",
    "    return weights @ value\n",
    "\n",
    "output = SelfAttention(K, Q, V, embeddings)\n",
    "output.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This way we can transform the embeddings using the surrounding embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Attention\n",
    "\n",
    "The Cross-Attention is a bridge, connecting the encoder and the decoder. The encoder summarizes the input and the decoder uses that information to produce the final output.\n",
    "\n",
    "In this process:\n",
    "- The **keys** and **values** are the encoder's output, which hold all the important details about the input.\n",
    "- The **query** is from the decoder's previous layer, asking: \"What part of the whole sequence should I pay attention to right now?\"\n",
    "\n",
    "In other words, it helps the decoder to decide which parts of the encoder's output are most relevant for generating the next word in the sequence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Head Attention\n",
    "\n",
    "Instead of using a single attention mechanism, the model applies multiple attention heads in parallel. Each head performs independent qurey, key and value projections with learned weights. The outputs from all heads are concatenated and then projected again.\n",
    "\n",
    "Splitting attention into multiple heads allows the model to focus on different aspects of the input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Example: Multi-Head Attention\n",
    "class MultiHeadAttentionExample(torch.nn.Module):\n",
    "    def __init__(self, embed_size, num_heads):\n",
    "        super(MultiHeadAttentionExample, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_size // num_heads\n",
    "\n",
    "        assert (\n",
    "            self.head_dim * num_heads == embed_size\n",
    "        ), \"Embedding size must be divisible by the number of heads\"\n",
    "\n",
    "        self.query = torch.nn.Linear(embed_size, embed_size)\n",
    "        self.key = torch.nn.Linear(embed_size, embed_size)\n",
    "        self.value = torch.nn.Linear(embed_size, embed_size)\n",
    "\n",
    "        # Final linear layer after concatenating heads\n",
    "        self.fc_out = torch.nn.Linear(embed_size, embed_size)\n",
    "\n",
    "    def forward(self, values, keys, queries):\n",
    "        batch_size = queries.size()[0]\n",
    "        query_len, key_len, value_len = queries.size()[1], keys.size()[1], values.size()[1]\n",
    "\n",
    "        queries = self.query(queries)  # (batch_size, query_len, embed_size)\n",
    "        keys = self.key(keys)          # (batch_size, key_len, embed_size)\n",
    "        values = self.value(values)    # (batch_size, value_len, embed_size)\n",
    "\n",
    "        # Split embeddings for each head\n",
    "        queries = queries.view(batch_size, query_len, self.num_heads, self.head_dim)\n",
    "        keys = keys.view(batch_size, key_len, self.num_heads, self.head_dim)\n",
    "        values = values.view(batch_size, value_len, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Scaled Dot-Product Attention\n",
    "        queries = queries.transpose(1, 2)  # (batch_size, num_heads, query_len, head_dim)\n",
    "        keys = keys.transpose(1, 2)        # (batch_size, num_heads, key_len, head_dim)\n",
    "        values = values.transpose(1, 2)    # (batch_size, num_heads, value_len, head_dim)\n",
    "\n",
    "        similarities = torch.matmul(queries, keys.transpose(-1, -2)) / (self.head_dim ** 0.5)\n",
    "        attention_weights = torch.softmax(similarities, dim=-1)  # (batch_size, num_heads, query_len, key_len)\n",
    "\n",
    "        out = torch.matmul(attention_weights, values)  # (batch_size, num_heads, query_len, head_dim)\n",
    "\n",
    "        # Concatenate heads\n",
    "        out = out.transpose(1, 2).contiguous()  # (batch_size, query_len, num_heads, head_dim)\n",
    "        out = out.view(batch_size, query_len, -1)  # (batch_size, query_len, embed_size)\n",
    "\n",
    "        out = self.fc_out(out)  # (batch_size, query_len, embed_size)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 8])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_size = 8\n",
    "num_heads = 2\n",
    "multi_head_attention = MultiHeadAttentionExample(embed_size, num_heads)\n",
    "\n",
    "# Random input\n",
    "batch_size, seq_len = 1, 3\n",
    "values = torch.rand(batch_size, seq_len, embed_size)\n",
    "keys = torch.rand(batch_size, seq_len, embed_size)\n",
    "queries = torch.rand(batch_size, seq_len, embed_size)\n",
    "\n",
    "output = multi_head_attention(values, keys, queries)\n",
    "output.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding\n",
    "\n",
    "We want to use the order of the words in the sequence, because it matters whether \"The dog chased the cat.\" or \"The cat chased the dog.\". We use the following formula to encode the positions:\n",
    "\n",
    "$\\text{PE}_{(pos,2i)} = \\sin{(pos/10000^{2i/d_{\\text{model}}​})}$\n",
    "\n",
    "$\\text{PE}_{(pos,2i+1)} = \\cos{(pos/10000^{2i/d_{\\text{model}}})}$\n",
    "\n",
    "where\n",
    "* $pos$ = token position\n",
    "* $i$ = embedding dimension index\n",
    "* $d_{model}$ = embedding size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sin, cos\n",
    "\n",
    "pos_encoding = torch.zeros_like(embeddings)\n",
    "seq_len, d_model = pos_encoding.size()\n",
    "\n",
    "for pos, embedding in enumerate(embeddings):\n",
    "    for i, dim in enumerate(embedding):\n",
    "        if i % 2 == 0:\n",
    "            pos_encoding[pos, i] = sin(pos/(10000**(2*i/d_model)))\n",
    "        else:\n",
    "            pos_encoding[pos, i] = cos(pos/(10000**(2*i/d_model)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(pos_encoding[:, :80], cmap=\"viridis\", xticklabels=5, yticklabels=1)\n",
    "plt.xlabel(\"Embedding Dimensions\")\n",
    "plt.ylabel(\"Token Positions\")\n",
    "plt.title(\"Positional Encoding Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/tutorials/intermediate/transformer_building_blocks.html\n",
    "# https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
    "\n",
    "# https://medium.com/@h.nossraty/a-comprehensive-guide-to-pytorchs-nn-transformer-module-a-morse-code-translation-example-8b033f866b18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# French - English\n",
    "# https://pytorch.org/tutorials/beginner/torchtext_translation_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_token = 0\n",
    "SOS_token = 1\n",
    "EOS_token = 2\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.n_words = 3  # Count SOS, EOS and PAD\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n",
    "    return s.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, path, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    with open(path, encoding='utf-8') as file:\n",
    "        lines = file.read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')[:2]] for l in lines]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \",\n",
    ")\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) <= MAX_LENGTH and \\\n",
    "           len(p[1].split(' ')) <= MAX_LENGTH and \\\n",
    "           p[1].startswith(eng_prefixes)\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareData(lang1, lang2, path,reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, path, reverse)\n",
    "\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    \n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    \n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    \n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "# input_lang, output_lang, pairs = prepareData('eng', 'hun', \"../data/translation/hun.txt\", True)\n",
    "# print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "def get_dataloader(batch_size):\n",
    "    input_lang, output_lang, pairs = prepareData('eng', 'hun', \"../data/translation/hun.txt\", True)\n",
    "\n",
    "    n = len(pairs)\n",
    "    input_ids = np.full((n, MAX_LENGTH+2), fill_value=PAD_token, dtype=np.int32)\n",
    "    target_ids = np.full((n, MAX_LENGTH+2), fill_value=PAD_token, dtype=np.int32)\n",
    "\n",
    "    inx = np.arange(len(pairs))\n",
    "    np.random.shuffle(inx)\n",
    "\n",
    "    for idx, (inp, tgt) in enumerate(pairs):\n",
    "        inp_ids = indexesFromSentence(input_lang, inp)\n",
    "        tgt_ids = indexesFromSentence(output_lang, tgt)\n",
    "        inp_ids.append(EOS_token)\n",
    "        tgt_ids.append(EOS_token)\n",
    "        inp_ids = [SOS_token] + inp_ids\n",
    "        tgt_ids = [SOS_token] + tgt_ids\n",
    "        input_ids[idx, :len(inp_ids)] = inp_ids\n",
    "        target_ids[idx, :len(tgt_ids)] = tgt_ids\n",
    "\n",
    "    train_input_ids = input_ids[inx[:8000]]\n",
    "    val_input_ids = input_ids[inx[8000:]]\n",
    "    train_target_ids = target_ids[inx[:8000]]\n",
    "    val_target_ids = target_ids[inx[8000:]]\n",
    "\n",
    "    train_data = TensorDataset(torch.LongTensor(train_input_ids).to(device),\n",
    "                               torch.LongTensor(train_target_ids).to(device))\n",
    "    val_data = TensorDataset(torch.LongTensor(val_input_ids).to(device),\n",
    "                               torch.LongTensor(val_target_ids).to(device))\n",
    "\n",
    "    train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return input_lang, output_lang, train_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=15):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        self.encoding = torch.zeros(max_len, d_model)\n",
    "        \n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "\n",
    "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.encoding = self.encoding.unsqueeze(0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "            return x + self.encoding[:, :x.size(1), :].to(x.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_vocab_size, target_vocab_size, d_model=512, nhead=8, num_layers=3, dim_feedforward=2048):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Embedding layers\n",
    "        self.input_embedding = nn.Embedding(input_vocab_size, d_model)\n",
    "        self.target_embedding = nn.Embedding(target_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "\n",
    "        # Transformer\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model, nhead=nhead, num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers, dim_feedforward=dim_feedforward, dropout=0.1\n",
    "        )\n",
    "\n",
    "        # Output layer\n",
    "        self.fc_out = nn.Linear(d_model, target_vocab_size)\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
    "        return mask\n",
    "    \n",
    "    def forward(self, src, tgt):\n",
    "        src_mask = None\n",
    "        tgt_mask = self.generate_square_subsequent_mask(tgt.size(1)).to(src.device)\n",
    "        \n",
    "        src_emb = self.positional_encoding(self.input_embedding(src) * np.sqrt(self.d_model))\n",
    "        tgt_emb = self.positional_encoding(self.target_embedding(tgt) * np.sqrt(self.d_model))\n",
    "\n",
    "        output = self.transformer(src_emb.transpose(0, 1), tgt_emb.transpose(0, 1), src_mask=src_mask, tgt_mask=tgt_mask)\n",
    "\n",
    "        return self.fc_out(output.transpose(0, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_dataloader, criterion, device='cpu'):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in val_dataloader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            tgt_input = tgt[:, :-1]\n",
    "            tgt_output = tgt[:, 1:]\n",
    "\n",
    "            output = model(src, tgt_input)\n",
    "\n",
    "            loss = criterion(output.reshape(-1, output.size(-1)), tgt_output.reshape(-1))\n",
    "            total_loss += loss.item()\n",
    "            pred = output.argmax(2)\n",
    "            correct += (pred == tgt_output).sum().item()\n",
    "            total += tgt_output.numel()\n",
    "\n",
    "    val_loss = total_loss / len(val_dataloader)\n",
    "    val_acc = correct / total\n",
    "\n",
    "    # print(f\"Validation Loss: {val_loss:.4f}, Accuracy: {val_acc:.4f}\")\n",
    "    return val_loss, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataloader, val_dataloader, optimizer, criterion, num_epochs=10, device=\"cpu\", print_every=100):    \n",
    "    train_losses, val_losses = [], []\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for src, tgt in train_dataloader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            tgt_input = tgt[:, :-1]  # Decoder input\n",
    "            tgt_output = tgt[:, 1:]  # Ground truth\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(src, tgt_input)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(output.reshape(-1, output.size(-1)), tgt_output.reshape(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # Validation\n",
    "        val_loss, val_acc = evaluate(model, val_dataloader, criterion, batch_size=32, device=device)\n",
    "        \n",
    "        val_losses.append(val_loss)\n",
    "        train_losses.append(total_loss / len(train_dataloader))\n",
    "        if epoch+1 % print_every == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_dataloader):.4f} - Val Loss: {val_loss:.4f} - Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "\n",
    "input_lang, output_lang, train_dataloader, val_dataloader = get_dataloader(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[128], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m)\n\u001b[1;32m     19\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(ignore_index\u001b[38;5;241m=\u001b[39mpadding_id)\n\u001b[0;32m---> 21\u001b[0m train_losses, val_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[127], line 12\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_dataloader, val_dataloader, optimizer, criterion, num_epochs, device, print_every)\u001b[0m\n\u001b[1;32m      9\u001b[0m tgt_output \u001b[38;5;241m=\u001b[39m tgt[:, \u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# Ground truth\u001b[39;00m\n\u001b[1;32m     11\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 12\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[1;32m     15\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, output\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)), tgt_output\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[121], line 31\u001b[0m, in \u001b[0;36mTransformerModel.forward\u001b[0;34m(self, src, tgt)\u001b[0m\n\u001b[1;32m     28\u001b[0m src_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_encoding(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_embedding(src) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_model))\n\u001b[1;32m     29\u001b[0m tgt_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_encoding(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_embedding(tgt) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_model))\n\u001b[0;32m---> 31\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_emb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_emb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_out(output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:145\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, src, tgt, src_mask, tgt_mask, memory_mask, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m src\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_model \u001b[38;5;129;01mor\u001b[39;00m tgt\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_model:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe feature number of src and tgt must be equal to d_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 145\u001b[0m memory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(tgt, memory, tgt_mask\u001b[38;5;241m=\u001b[39mtgt_mask, memory_mask\u001b[38;5;241m=\u001b[39mmemory_mask,\n\u001b[1;32m    147\u001b[0m                       tgt_key_padding_mask\u001b[38;5;241m=\u001b[39mtgt_key_padding_mask,\n\u001b[1;32m    148\u001b[0m                       memory_key_padding_mask\u001b[38;5;241m=\u001b[39mmemory_key_padding_mask)\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:315\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    312\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m make_causal\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 315\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_key_padding_mask_for_layers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_nested:\n\u001b[1;32m    318\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_padded_tensor(\u001b[38;5;241m0.\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:591\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[0;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    589\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x))\n\u001b[1;32m    590\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 591\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sa_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    592\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(x))\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:599\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._sa_block\u001b[0;34m(self, x, attn_mask, key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sa_block\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor,\n\u001b[1;32m    598\u001b[0m               attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor], is_causal: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 599\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    603\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/activation.py:1205\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1191\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1192\u001b[0m         query, key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1202\u001b[0m         average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights,\n\u001b[1;32m   1203\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[1;32m   1204\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1205\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1206\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1207\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1208\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1209\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1211\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1212\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1214\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1215\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[1;32m   1217\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/functional.py:5373\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   5370\u001b[0m k \u001b[38;5;241m=\u001b[39m k\u001b[38;5;241m.\u001b[39mview(bsz, num_heads, src_len, head_dim)\n\u001b[1;32m   5371\u001b[0m v \u001b[38;5;241m=\u001b[39m v\u001b[38;5;241m.\u001b[39mview(bsz, num_heads, src_len, head_dim)\n\u001b[0;32m-> 5373\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5374\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(bsz \u001b[38;5;241m*\u001b[39m tgt_len, embed_dim)\n\u001b[1;32m   5376\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m linear(attn_output, out_proj_weight, out_proj_bias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Instantiate the model\n",
    "model = TransformerModel(\n",
    "    input_vocab_size=input_lang.n_words,\n",
    "    target_vocab_size=output_lang.n_words,\n",
    "    d_model=256,  # Reduced for faster training\n",
    "    nhead=8,\n",
    "    num_layers=5,\n",
    "    dim_feedforward=512  # Reduced for simplicity\n",
    ").to(device)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "padding_id = PAD_token\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=padding_id)\n",
    "\n",
    "train_losses, val_losses = train(model, train_dataloader, val_dataloader, optimizer, criterion, num_epochs=10, device=device, print_every=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MorseDataset:\n",
    "    def __init__(self):\n",
    "        self.MORSE_CODE_DICT = {\n",
    "            'A': '01', 'B': '1000', 'C': '1010', 'D': '100', 'E': '0',\n",
    "            'F': '0010', 'G': '110', 'H': '0000', 'I': '00', 'J': '0111',\n",
    "            'K': '101', 'L': '0100', 'M': '11', 'N': '10', 'O': '111',\n",
    "            'P': '0110', 'Q': '1101', 'R': '010', 'S': '000', 'T': '1',\n",
    "            'U': '001', 'V': '0001', 'W': '011', 'X': '1001', 'Y': '1011',\n",
    "            'Z': '1100'\n",
    "        }\n",
    "        self.SOS_TOKEN = '<SOS>'\n",
    "        self.EOS_TOKEN = '<EOS>'\n",
    "        self.PADDING_TOKEN = '<PAD>'\n",
    "\n",
    "        morse_vocab_items = [self.PADDING_TOKEN, self.SOS_TOKEN, self.EOS_TOKEN, '0', '1']\n",
    "        self.morse_vocab = {ch: idx for idx, ch in enumerate(morse_vocab_items)}\n",
    "\n",
    "        text_vocab_items = [self.PADDING_TOKEN, self.SOS_TOKEN, self.EOS_TOKEN] + list(self.MORSE_CODE_DICT.keys())\n",
    "        self.text_vocab = {ch: idx for idx, ch in enumerate(text_vocab_items)}\n",
    "        self.reverse_text_vocab = {idx: ch for ch, idx in self.text_vocab.items()}\n",
    "\n",
    "\n",
    "    def generate_dataset(self, num_seq=1000, max_word_len=7):\n",
    "        \"\"\"Generates random Morse code sequences with text translations.\"\"\"\n",
    "        dataset = []\n",
    "        for _ in range(num_seq):\n",
    "            word_len = np.random.randint(1, max_word_len)\n",
    "            word = np.random.choice(list(self.MORSE_CODE_DICT.keys()), size=(word_len))\n",
    "            word = ''.join(word)\n",
    "            \n",
    "            morse_seq = self.text2morse(word)\n",
    "            dataset.append((morse_seq, word))\n",
    "\n",
    "        return dataset\n",
    "    \n",
    "    def dataset_from_file(self, file_path):\n",
    "        \"\"\"Reads Morse code sequences and their text translations from a file.\"\"\"\n",
    "        dataset = []\n",
    "        with open(file_path, 'r') as file:\n",
    "            text = file.read()\n",
    "\n",
    "            text = text.upper()\n",
    "            text = re.sub(r'[^A-Z\\s]', '', text)\n",
    "            \n",
    "            dataset = [(self.text2morse(word), word) for word in text.split() if len(word) <= 7 and len(word) >= 2]\n",
    "            \n",
    "            \n",
    "        return dataset\n",
    "    \n",
    "    def text2morse(self, text):\n",
    "        \"\"\"Converts text to Morse code.\"\"\"\n",
    "        morse_seq = \"\".join([self.MORSE_CODE_DICT[char] for char in text])\n",
    "        return morse_seq\n",
    "\n",
    "    def tokenized_data(self, batch):\n",
    "        \"\"\"Converts sequences into padded tensors with indexes.\"\"\"\n",
    "        \n",
    "        max_morse_len, max_text_len = -1, -1\n",
    "        for morse_seq, text_seq in batch:\n",
    "            max_morse_len = max(max_morse_len, len(morse_seq))\n",
    "            max_text_len = max(max_text_len, len(text_seq))\n",
    "\n",
    "        max_morse_len += 2  # Add 2 for the SOS, EOS token\n",
    "        max_text_len += 2  # Add 2 for the SOS, EOS token\n",
    "        morse_sequences, text_sequences = [], []\n",
    "\n",
    "        for morse_seq, text_seq in batch:\n",
    "                morse_tensor = [self.morse_vocab[self.SOS_TOKEN]] + \\\n",
    "                               [self.morse_vocab[ch] for ch in morse_seq] + \\\n",
    "                               [self.morse_vocab[self.EOS_TOKEN]]\n",
    "                \n",
    "                text_tensor = [self.text_vocab[self.SOS_TOKEN]] + \\\n",
    "                              [self.text_vocab[ch] for ch in text_seq] + \\\n",
    "                              [self.text_vocab[self.EOS_TOKEN]]\n",
    "                \n",
    "                morse_tensor += [self.morse_vocab[self.PADDING_TOKEN]] * (max_morse_len - len(morse_tensor))\n",
    "                text_tensor += [self.text_vocab[self.PADDING_TOKEN]] * (max_text_len - len(text_tensor))\n",
    "                morse_sequences.append(morse_tensor)\n",
    "                text_sequences.append(text_tensor)\n",
    "\n",
    "        return torch.tensor(morse_sequences), torch.tensor(text_sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('100000', 'THE'),\n",
       " ('10100111001001011', 'TRAGEDY'),\n",
       " ('1110010', 'OF'),\n",
       " ('010111110111', 'ROMEO'),\n",
       " ('0110100', 'AND'),\n",
       " ('011100101000001', 'JULIET'),\n",
       " ('10001011', 'BY'),\n",
       " ('0110001000100000111', 'WILLIAM'),\n",
       " ('10100000111010001000', 'CHORUS'),\n",
       " ('00001010010100001000', 'ESCALUS')]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shakespeare = MorseDataset()\n",
    "dataset = shakespeare.dataset_from_file(\"../data/shakespeare.txt\")\n",
    "dataset[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('000001100000', 'HABEE'),\n",
       " ('0111', 'J'),\n",
       " ('01111011', 'JY'),\n",
       " ('1100110000011011011010', 'ZZVYPN'),\n",
       " ('0001', 'V'),\n",
       " ('101101', 'YA'),\n",
       " ('01111110101101011', 'JTQEGY'),\n",
       " ('00101011100101001110', 'FYDCJE'),\n",
       " ('10010000110100', 'XIUCE'),\n",
       " ('00011111100', 'VMMD')]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morse_dataset = MorseDataset()\n",
    "train_dataset = morse_dataset.generate_dataset(num_seq=10, max_word_len=7)\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1, 3, 4, 4, 4, 2, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([ 1, 12,  2,  0,  0,  0,  0]))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morse_sequences, text_sequences = morse_dataset.tokenized_data(train_dataset[:2])\n",
    "morse_sequences[1], text_sequences[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=50):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        self.encoding = torch.zeros(max_len, d_model)\n",
    "        \n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "\n",
    "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.encoding = self.encoding.unsqueeze(0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "            return x + self.encoding[:, :x.size(1), :].to(x.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_vocab_size, target_vocab_size, d_model=512, nhead=8, num_layers=3, dim_feedforward=2048):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Embedding layers\n",
    "        self.input_embedding = nn.Embedding(input_vocab_size, d_model)\n",
    "        self.target_embedding = nn.Embedding(target_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "\n",
    "        # Transformer\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model, nhead=nhead, num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers, dim_feedforward=dim_feedforward, dropout=0.1\n",
    "        )\n",
    "\n",
    "        # Output layer\n",
    "        self.fc_out = nn.Linear(d_model, target_vocab_size)\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
    "        return mask\n",
    "    \n",
    "    def forward(self, src, tgt):\n",
    "        src_mask = None\n",
    "        tgt_mask = self.generate_square_subsequent_mask(tgt.size(1)).to(src.device)\n",
    "        \n",
    "        src_emb = self.positional_encoding(self.input_embedding(src) * np.sqrt(self.d_model))\n",
    "        tgt_emb = self.positional_encoding(self.target_embedding(tgt) * np.sqrt(self.d_model))\n",
    "\n",
    "        output = self.transformer(src_emb.transpose(0, 1), tgt_emb.transpose(0, 1), src_mask=src_mask, tgt_mask=tgt_mask)\n",
    "\n",
    "        return self.fc_out(output.transpose(0, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_data, criterion, batch_size=32, device='cpu'):\n",
    "    val_morse, val_text = val_data\n",
    "    val_dataset = torch.utils.data.TensorDataset(val_morse, val_text)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size)\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in val_loader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            tgt_input = tgt[:, :-1]\n",
    "            tgt_output = tgt[:, 1:]\n",
    "            output = model(src, tgt_input)\n",
    "            loss = criterion(output.reshape(-1, output.size(-1)), tgt_output.reshape(-1))\n",
    "            total_loss += loss.item()\n",
    "            pred = output.argmax(2)\n",
    "            correct += (pred == tgt_output).sum().item()\n",
    "            total += tgt_output.numel()\n",
    "    val_loss = total_loss / len(val_loader)\n",
    "    val_acc = correct / total\n",
    "    # print(f\"Validation Loss: {val_loss:.4f}, Accuracy: {val_acc:.4f}\")\n",
    "    return val_loss, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data, val_data, scheduler, optimizer, criterion, num_epochs=10, batch_size=32, device=\"cpu\"):\n",
    "    train_morse, train_text = train_data\n",
    "    val_morse, val_text = val_data\n",
    "    \n",
    "    # Create DataLoader for training data\n",
    "    train_dataset = torch.utils.data.TensorDataset(train_morse, train_text)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for src, tgt in train_loader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            tgt_input = tgt[:, :-1]  # Decoder input\n",
    "            tgt_output = tgt[:, 1:]  # Ground truth\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(src, tgt_input)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(output.reshape(-1, output.size(-1)), tgt_output.reshape(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # Validation\n",
    "        val_loss, val_acc = evaluate(model, val_data, criterion, batch_size=32, device=device)\n",
    "        \n",
    "        losses.append(total_loss / len(train_loader))\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader):.4f} - Val Loss: {val_loss:.4f} - Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = morse_dataset.generate_dataset(num_seq=5000, max_word_len=7)\n",
    "# train_data = morse_dataset.tokenized_data(train_dataset)\n",
    "# val_dataset = morse_dataset.generate_dataset(num_seq=100, max_word_len=7)\n",
    "# val_data = morse_dataset.tokenized_data(val_dataset)\n",
    "\n",
    "\n",
    "train_dataset = shakespeare.dataset_from_file(\"../data/shakespeare.txt\")\n",
    "train_data = shakespeare.tokenized_data(train_dataset)\n",
    "val_dataset = morse_dataset.generate_dataset(num_seq=100, max_word_len=7)\n",
    "val_data = morse_dataset.tokenized_data(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[126], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(ignore_index\u001b[38;5;241m=\u001b[39mpadding_id)\n\u001b[1;32m     19\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mCosineAnnealingLR(optimizer, T_max\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m train_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[115], line 23\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_data, val_data, scheduler, optimizer, criterion, num_epochs, batch_size, device)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[1;32m     22\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, output\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)), tgt_output\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 23\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     25\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Instantiate the model\n",
    "model = TransformerModel(\n",
    "    input_vocab_size=len(morse_dataset.morse_vocab),\n",
    "    target_vocab_size=len(morse_dataset.text_vocab),\n",
    "    # d_model=256,  # Reduced for faster training\n",
    "    # nhead=8,\n",
    "    # num_layers=5,\n",
    "    # dim_feedforward=512  # Reduced for simplicity\n",
    ").to(device)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "padding_id = morse_dataset.text_vocab[morse_dataset.PADDING_TOKEN]\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=padding_id)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=15)\n",
    "\n",
    "train_losses = train(model, train_data, val_data, scheduler, optimizer, criterion, num_epochs=10, batch_size=32, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 3.2665, Accuracy: 0.0744\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3.2664764523506165, 0.07436619718309859)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model, val_data, criterion, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_metrics(history):\n",
    "    plt.plot(history[\"train_loss\"], label=\"Train Loss\")\n",
    "    plt.plot(history[\"val_loss\"], label=\"Validation Loss\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Loss Trends\")\n",
    "    plt.show()\n",
    "    plt.plot(history[\"train_acc\"], label=\"Train Accuracy\")\n",
    "    plt.plot(history[\"val_acc\"], label=\"Validation Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Accuracy Trends\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sequence(sequence, model, morse_dataset):\n",
    "    model.eval()\n",
    "    morse_tensor = [morse_dataset.morse_vocab[ch] for ch in sequence if ch in morse_dataset.morse_vocab]\n",
    "    morse_tensor = torch.tensor(morse_tensor, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    tgt_input = torch.tensor([morse_dataset.text_vocab[morse_dataset.SOS_TOKEN]], dtype=torch.long).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        for _ in range(20):\n",
    "            output = model(morse_tensor, tgt_input)\n",
    "            next_token = output.argmax(2)[:, -1:]\n",
    "            tgt_input = torch.cat((tgt_input, next_token), dim=1)\n",
    "            if next_token.item() == morse_dataset.text_vocab[morse_dataset.EOS_TOKEN]:\n",
    "                break\n",
    "    translated_text = ''.join(morse_dataset.reverse_text_vocab[idx.item()] for idx in tgt_input[0, 1:-1])\n",
    "    return translated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'F F F F F F F F F F'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence = \"0101001101\"\n",
    "\n",
    "translate_sequence(sequence, model, morse_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
